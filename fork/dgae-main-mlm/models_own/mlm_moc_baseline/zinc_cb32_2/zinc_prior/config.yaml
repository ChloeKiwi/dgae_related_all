_wandb:
    value:
        cli_version: 0.19.0
        m: []
        python_version: 3.8.20
        t:
            "1":
                - 1
                - 5
                - 53
                - 55
                - 77
            "2":
                - 1
                - 5
                - 53
                - 55
                - 77
            "3":
                - 3
                - 13
                - 14
                - 16
                - 23
                - 55
            "4": 3.8.20
            "5": 0.19.0
            "8":
                - 5
            "12": 0.19.0
            "13": linux-x86_64
autoencoder_path:
    value: ./wandb/enzymes_prior/files/config.yaml
data:
    value:
        add_cycles_feat: true
        add_path_feat: true
        add_random_feat: true
        add_spectral_feat: true
        data: ZINC250k
        dir: ./data
        max_feat_num: 9
        max_node_num: 38
dataset:
    value: zinc
device:
    value: cuda:3
exp_name:
    value: mlm_moc_baseline
gpu:
    value: "3"
log:
    value:
        debug: false
        n_loggin_epochs: 1
        n_loggin_steps: 2000
        wandb: online
model:
    value:
        decoder:
            mlp_hidden_size: 128
            mlp_n_layers: 3
            n_layers: 4
            name: GNN
            nhf: 32
            normalization: batch_norm
            skip_connection: true
        encoder:
            add_input_noise: false
            mlp_hidden_size: 128
            mlp_n_layers: 3
            n_layers: 4
            name: GNN_PYG
            nhf: 32
            normalization: batch_norm
            skip_connection: true
        gamma: 0.1
        quantizer:
            codebook_size: 32
            commitment_cost: 0.25
            decay: 0.99
            epsilon: 1e-05
            init_steps: 0
            name: Quantizer
            nc: 2
            nz: 8
model_folder:
    value: ./wandb/enzymes_prior/files/config.yaml
run_name:
    value: zinc_cb32_2
sample:
    value: false
train_prior:
    value: true
training:
    value:
        batch_size: 32
        beta1: 0.9
        beta2: 0.99
        betas:
            - 0.9
            - 0.99
        decay_iteration: 25000
        epochs: 40
        learning_rate: 0.0005
        lr_decay: 0.5
        n_iter: 1000000
        sort_indices: true
transformer:
    value:
        d_model: 256
        n_blocks: 6
        num_heads: 16
use_mask:
    value: false
work_type:
    value: train_prior
