_wandb:
    value:
        cli_version: 0.19.4
        m: []
        python_version: 3.8.10
        t:
            "1":
                - 1
                - 5
                - 53
                - 55
                - 77
            "2":
                - 1
                - 5
                - 53
                - 55
                - 77
            "3":
                - 3
                - 13
                - 14
                - 16
                - 23
                - 55
            "4": 3.8.10
            "5": 0.19.4
            "8":
                - 5
            "12": 0.19.4
            "13": linux-x86_64
data:
    value:
        add_cycles_feat: true
        add_path_feat: true
        add_random_feat: true
        add_spectral_feat: true
        batch_size: 8
        data: grid
        dir: ./data
        max_feat_num: 5
        max_node_num: 342
        test_split: 0.2
dataset:
    value: grid
device:
    value: cpu
exp_name:
    value: dgae_baseline_different_hyper
gpu:
    value: "0"
log:
    value:
        debug: false
        n_loggin_epochs: 100
        n_loggin_steps: 1000
        wandb: online
model:
    value:
        decoder:
            mlp_hidden_size: 64
            mlp_n_layers: 3
            n_layers: 2
            name: GNN
            nhf: 32
            normalization: batch_norm
            skip_connection: true
        encoder:
            mlp_hidden_size: 64
            mlp_n_layers: 3
            n_layers: 2
            name: GNN_PYG
            nhf: 32
            normalization: batch_norm
            skip_connection: true
        gamma: 0.1
        quantizer:
            codebook_size: 64
            commitment_cost: 0.25
            decay: 0.99
            epsilon: 1e-05
            init_steps: 0
            name: Quantizer
            nc: 2
            nz: 8
model_folder:
    value: ./wandb/enzymes_prior/files/config.yaml
run_name:
    value: grid_cb64_2
sample:
    value: false
train_autoencoder_skip:
    value: false
train_prior:
    value: false
training:
    value:
        batch_size: 32
        beta1: 0.9
        beta2: 0.99
        betas:
            - 0.9
            - 0.99
        decay_iteration: 10000
        epochs: 10000
        learning_rate: 0.001
        log: false
        lr_decay: 0.5
        n_iter: 1000000
work_type:
    value: train_autoencoder
