
training:
  # batch_size: 32
  batch_size: 64
  learning_rate: 0.0003
  lr_decay: 0.7
  decay_iteration: 1500
  beta1: 0.9
  beta2: 0.999
  n_iter: 500_000
  epochs: 2500
  sort_codebook: False
  sort_indices: True

transformer:
  d_model: 64
  # d_model: 128
  num_heads: 16
  # n_blocks: 3
  n_blocks: 6

log:
    n_loggin_steps: 1000
    n_loggin_epochs: 100
    wandb: online
    debug: False